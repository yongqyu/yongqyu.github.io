---
layout: post
category: develop
comments_id: 2
---
[link](https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective?utm_source=reddit&utm_medium=post&utm_campaign=blog-mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective&utm_content=deeplearning)  
neptune.ai Blog     
Jan 14, 2021

이 글은 비공식 번역문 입니다.

*[previous post 1](https://yongqyu.github.io/mldl-mlops-what-it-is-why-it-matters-and-how-to-implement-it-1.html)*  

-----------------------------------------------------

## How to implement MLOps

MLOps를 구현하는 세가지 방법이 있다.:

* MLOps level 0 (Manual process)
* MLOps level 1 (ML pipeline automation)
* MLOps level 2 (CI/CD pipeline automation).
  
### MLOps level 0

이것은 ML을 막 시작한 회사들의 전형이다. 전체적으로 수작업 ML 워크플로우와 데이터 사이언티스트 기반의 처리는 모델이 거의 바뀌지 않거나 학습되지 않는다면 충분하다.

#### **Characteristics**

* **수작업, 스크립트 기반 그리고 상호적 프로세스**: 데이터 분석, 준비, 모델 학습 및 평가 등 모든 스텝이 수작업이다. 이는 매 단계에서 수동으로 실행하고 다음스탭으로 직접 넘어간다.
* **ML과 운영간의 단절**: 모델을 만드는 데이터 사이언티스트와 예측 모델을 서빙하는 엔지니어가 구분돼 있다. 데이터 사이언티스트가 학습된 데이터 결과물을 엔지니어 팀에게 전달해 그들의 API 인프라구조에 배치한다.
* **비주기적 릴리즈 이터레이션**: 데이터 사이언티스트 팀이 잘 바뀌지 않는 (구현의 변화나 새로운 데이터로의 학습) 적은 수의 모델을 관리하는 경우가 해당한다. 새로운 모델 버전이 1년에 몇번 없는 경우이다.
* **No CI**: 구현 변화가 거의 없기 때문에, 지속적 통합은 무시한다. 주로, 코드 테스트는 notebooks나 스크립트를 통해 실행한다.
* **No CD**: 주기적으로 모델을 배포하지 않기 때문에, 지속적 배포는 고려되지 않는다. 
* **배포가 서비스를 참조**한다. (예. REST API를 통한 마이크로서비스)
* **능동적인 성능 모니터링 결여**: 프로세스는 모델의 예측이나 액션을 쫓거나 로그로 남기지 않는다.

엔지니어링 팀은 보안, 회귀 그리고 로드 + 카나리아 테스트를 포함한 API 설정, 테스트, 배포를 위한 자체적인 복잡한 설정을 가지고 있을 것이다.

#### **Challenges**

실전에서는, 모델을 배포했을 때 자주 죽는다. 모델은 다양한 환경이나 환경을 나타내는 데이터의 변화를 받아들이는데 실패한다. 이에 관한 포브스의 게시글: [Why Machine Learning Models Crash and Burn in Production.](https://www.forbes.com/sites/forbestechcouncil/2019/04/03/why-machine-learning-models-crash-and-burn-in-production/)

이런 수작업의 문제를 해결하기 위해, CI/CD 그리고 CT를 위한 MLOps를 쓰는 것이 좋다. ML 학습 파이프라인 배치로 인해, CT가 가능하고 빠른 테스트, 빌드, 새로운 구현의 ML 파이프라인을 배치를 위한 CI/CD 시스템도 설정할 수 있다.

### MLOps level 1

MLOps level 1의 목표는 ML 파이프라인을 자동화해 모델의 지속적 학습 (CT) 을 수행하는 것이다. 이 방법으로, 모델 예측 서비스의 지속적 제공이 가능하다. 이 시나리오는 지속적으로 변하는 환경에서 운영하거나, 사전에 사용자의 행동이나 가격 비율 그리고 다른 지표들의 변화에 대응하기를 원할 때 효과적이다.

#### **Characteristics**

* 빠른 실험: ML 실험 설정이 자동으로 조정되고 실행된다.
* 제품에서의 모델 CT: 모딜이 실시간 파이프라인 트리커를 통해 들어온 새로운 데이터로 자동 학습된다.
* 실험적-운영적 측면에서 균형: 개발이나 실험 환경에 쓰이는 파이프라인 구현은 통합된 데브옵스를 위한 가장 중요한 요소로써, 사전제품/제품 환경에서 사용된다.
* 구성 요소와 파이프라인을 위한 코드 모듈화: ML 파이프라인을 구축하기 위해, 구성 요소들은 재사용 가능하고, 구성 가능하고, ML 파이프라인간의 공유가 가능해야한다.
* 모델의 지속적 배포: 학습되고 증명된 모델을 예측 서비스로서 실시간으로 서비스 하는 모델 비치 단계는 자동화 되야 한다.
* 파이프라인 배치: 레벨 0에서는 학습된 모델을 예측 서비스로서 제품으로 배치했다. 레빌 1에서는 자동화되고 반복적으로 실행되는 전체 학습 파이프라인을 예측 서비스로서 제품으로 배치한다.

#### **Additional components**

* 데이터와 모델 검증: 새로운 데이터에서 학습된 새 버전의 모델을 만들기 위해 파이프라인은 새로운, 실시간의 데이터를 기대한다. 그래서 자동화된 데이터 검증과 모델 검증 단계가 필요하다.
* 피쳐 스토어(feature store): 피쳐 스토어는 학습 및 서빙을 위한 피쳐의 정의, 저장, 접근을 표준화한 중앙화된 저장소이다.
* 메타데이터 관리: ML 파이프라인의 각 실행에 대한 정보는 데이터와 결과물 버저닝과 재생산성 및 비교를 위해 기록된다. 이는 또한 에러를 잡거나 이상 탐지하는데 쓰인다.
* ML 파이프라인 트리거: 특정 케이스에 따라, ML 파이프라인을 새로운 데이터에 대한 재학습을 위해 자동화할 수 있다.:
  * 요구에 따라 (온디멘드, On-demand)
  * 스케쥴에 따라
  * 새로운 학습 가능 데이터에 따라
  * 모델 성능 저하에 따라
  * (데이터 프로파일 변화에 따른) 데이터 분포의 큰 변화에 따라

#### **Challenges**

이 설정은 새로운 ML 아이디어 기반 보다는, 새로운 데이터에 따른 새 모델을 대치할 때 적합하다. 하지만 새로운 ML 아이디어를 시도해보고 빠르게 ML 컴포넌트들의 구현을 배치하고 싶다면, 만약 많은 ML 파이프라인을 관리한다면, ML 파이프라인의 빌드, 테스트, 배포를 자동화 하기 위한 CI/CD 설정이 필요하다.

### MLOps level 2

빠르고 신뢰성 있는 파이프라인의 업데이트를 위해 탄탄하게 자동화된 CI/CD 시스템이 필요하다. 이 자동화된 CI/CD 시스템과 함께, 데이터 사이언티스트들은 피쳐 엔지니어링, 모델 구조, 하이퍼파라미터 등의 새로운 아이디어들을 빠르게 탐색할 수 있다. 이 단계는 매일 분단위로 그들의 모델을 재학습시키고 수천개의 서버에 동시에 재배포 해야하는 기술 기반의 회사에 적합하다. end-to-end MLOps 사이클이 아니면 이런 구조들은 살아남을 수 없다. 이런 MLOps의 설정은 다음 구성들을 포함한다.:

* 소스 컨트롤
* 테스트와 빌드 서비스
* 배포 서비스
* 모델 등록
* 피쳐 스토어
* ML 메타데이터 스토어
* ML 파이프라인 조율

#### **Characteristics** 

* **개발 및 실험**: 실험 설정이 조율된 상태에서 반복적으로 새로운 ML 알고리즘과 새 모델을 시도한다. 이 스테이지의 아웃풋은 소스 저장소에 올려져있는 ML 파이프라인 스텝들의 소스코드이다. 
* **파이프라인의 지속적 통합**: 소스 코드를 빌드하고 다양한 실험을 한다. 이 단계의 아웃풋은 다음 단계에서 배포될 파이프라인 구성들(패키지, 실행파일, 결과물)이다.
* **파이프라인의 지속적 제공**: CI 단계에서 생성된 결과물을 목표 환경에 배포한다. 이 단계의 아웃풋은 모델의 새로운 구현으로 배포된 파이프라인이다.
* **자동화된 트리거링**: 파이프라인은 스케줄이나 트리거의 응답에 의해 자동적으로 실행된다. 이 단계의 결과물은 등록된 새로 학습된 모델이다.
* **모델의 지속적 제공**: 학습된 모델을 예측 모델로 제공한다. 이 단계의 아웃풋은 배포된 예측 서비스의 모델이다.
* **모니터링**: 실시간 데이터에 따른 모델 선능의 통계치를 수집한다. 이 단게의 아웃풋은 파이프라인이나 새로운 실험 사이클을 실행시키는 트리거이다.

데이터 분석 단계는 여전히 데이터 사이언티스트에 의해 파이프라인이 새로운 실험 이터레이션이 시작되기 전에 수작업으로 진행된다.