---
layout: post
category: develop
comments_id: 2
---
[link](https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective?utm_source=reddit&utm_medium=post&utm_campaign=blog-mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective&utm_content=deeplearning)  
neptune.ai Blog     
Jan 14, 2021

이 글은 비공식 번역문 입니다.

*[previous post](https://yongqyu.github.io/mldl-mlops-what-it-is-why-it-matters-and-how-to-implement-it-1.html)*  

-----------------------------------------------------

### DevOps vs MLOps

![mlops](https://i0.wp.com/neptune.ai/wp-content/uploads/MLOps-DevOps.png?w=896&ssl=1)
*source:NealAnalytics*

데브옵스와 MLOps는 MLOps가 데브옵스의 원리에서 나왔기에 근본적으로는 비슷하다. 하지만 실행할 때는 좀 다르다.:

1. 데브옵스와 달리, **MLOps는 좀 더 실험적**이다. 데이터 사이언티스트들과 ML/DL 엔지니어들은 다양한 피처들 - 하이퍼파라미터, 파라미터, 모델들 - 을 바꿔봐야 할 뿐 아니라, 결과를 재현하기 위해 이 피처들과 데이터, 코드 베이스들을 잘 트랙킹 해야한다. 모든 노력과 도구들에도 불구하고, ML/DL 산업은 여전히 결과 재구현 문제에 어려움을 겪고 있다. 이 문제는 이 글의 범위를 넘어서기에, 이 글에서 다루지는 않는다.  
2. **다양한 팀 구성**: 상품을 위해 모델을 개발하고 배포하는 팀을 구성할때 소프트웨어 엔지니어들로만 구성하지는 않는다. ML 프로젝트에서는, 일반적으로 데이터 분석, 모델 개발 및 실험에 초점을 맞춘 데이터 사이언티스트 또는 ML 연구원들이 포함된다. 그들은 상품 수준의 서비스를 개발한 적 있는 소프트웨어 엔지니어가 아닐 가능성이 높다.
3. **테스트**: ML 시스템을 테스트하는 것은 모델 평가, 모델 학습과 같은 것들 (그리고 유닛 테스트나 통합 테스트 같은 기존의 코드 테스트들) 이 포함된다.
4. **자동화된 배포**: 오프라인에서 학습된 ML 모델을 예측 서비스로 배포할수는 없다. 다양한 단계의 자동화된 모델 재학습 및 배포 파이프라인이 필요하다. 이 파이프라인은 복잡도를 더하는데, 왜냐하면 이 단계들은 원래 데이터 사이언티스트들이 수작업으로 진행하던 학습부터 평가 단계를 자동화 해야하기 때문이다.
5. **제품 성능이 데이터 프로파일 또는 단순한 학습-서빙 쏠림으로 저하**된다.: 제품에 있는 ML 모델들은 코드 최적화 문제로 성능이 낮아질 수 있을 뿐 아니라, 계속 발달하는 데이터 프로파일 문제로도 야기된다. 모델은 기존 소프트웨어 시스템보다 더 많은 이유들로 성능이 저하될 수 있기 때문에, 이를 대비해야 한다. 이런 이유들은 다음과 같다.: 
* 학습 시와 서비스 시의 데이터 처리 방법 차이
* 학습 시와 서비스 시의 데이터 변화
* 피드백 루프 - 최적화 하려는 가설이 잘못 된 경우, 학습을 위해 편향된 데이터를 수집하게 된다. 그러면 알아차리지 못한채, 미래 버전의 재학습과 최적화에 다시 들어가게 되고 모델을 점점 더 편항되게 된다.
6. **모니터링**: 제품의 모델들은 모니터링 되고 있어야 한다. 비슷하게, 데이터의 요약된 통계값도 적절한 시기에 모델을 바꿀 수 있도록 관찰되고 있어야 한다. 이런 통계들은 시간이 지남에 따라 변하고, 이에 따른 알림을 받거나 예상치를 벗어날 경우 프로세스를 되돌려야 한다.

MLOps와 데브옵스는 소스 컨트롤의 지속적인 통합, 유닛 테스트, 통합 테스트 그리고 소프트웨어 모듈이나 패키지의 지속적인 제공을 한다는 면에서는 유사하다. 하지만 ML에서는 눈여겨 볼만한 차이점들이 있다.

* **지속적인 통합 (CI)** 는 더이상 코드와 구성요소를 테스트하고 검증하는 것만이 아니다. 데이터, 데이터 스키마, 모델들도 테스트와 검증이 필요하다.
* **지속적 제공 (CD)** 는 더이상 하나의 소프트웨어 페키지나 서비스에 관한 것이 아닌, 자동으로 다른 서비스 (모델 예측 서비스) 를 제공하거나 변화를 되돌리는 시스템 (ML 학습 파이프라인) 과도 관련이 있다.
* **지속적 평가 (CT)** 는 ML 시스템의 특징적인 새 요소로, 자동화 된 재학습과 모델 서빙에 관한 것이다.

![mlops_pipe](https://i2.wp.com/neptune.ai/wp-content/uploads/ML-process.png?resize=1024%2C389&ssl=1)

### MLOps vs Experiment Tracking vs ML Model Management

MLOps가 무엇인지 알아봤는데, 그럼 실험 추적과 ML 모델 관리는 무엇인가.

#### **Experiment Tracking**

실험 추적은 MLOps의 한 파트로써, 다양한 환경 (하이퍼파라미터, 모델 크기, 데이터 구분 방법, 파라미터 등) 에서 여러번의 실험에 걸친 모델 학습 정보를 수집, 구성, 추적하는데에 초점이 있다. 이미 언급했듯이, ML/DL은 원래 매우 실험적이기 때문에, 다른 모델들을 비교하기 위해 실험 추적 도구를 쓴다.

#### **Model Management**

ML 모델이 일관되도록 하기 위해, 그리고 모든 비즈니스 요구사항과 확장성 면에서 부합하도록, 모델 관리를 위한 논리적이고 따라하기 쉬운 정책이 필수적이다. MLOps 방법론은 모델 학습, 패키징, 검증, 배포 그리고 모니터링을 간소화하는 과정이 포함한다. 이러면 처음부터 끝까지 인관되게 ML 프로젝트를 수행할 수 있다. 설정을 깔끔하게 하기 위해, 모델 관리를 위한 방법론은 이렇게 구성할 수 있다.:

* 공통된 비즈니스 고민들을 사전에 해결
* 데이터, 모델, 코드 그리고 모델 버전을 트랙킹 함으로 모델 재구현 가능
* 재사용성을 위해 반복되는 설정에서의 모델 구성 및 배포
